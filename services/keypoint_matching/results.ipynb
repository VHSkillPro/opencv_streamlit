{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hazzu/Code/opencv_streamlit\n"
     ]
    }
   ],
   "source": [
    "%cd \"../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from services.semantic_keypoint_detection.services import DATATYPES\n",
    "\n",
    "\n",
    "def read_image(type: int, name: str):\n",
    "    \"\"\"\n",
    "    Reads an image and its ground truth keypoints from the specified dataset.\n",
    "\n",
    "    :param type: The type of the dataset.\n",
    "    :param name: The name of the image.\n",
    "\n",
    "    :return (image, ground_truth): A tuple containing the image and the ground truth keypoints.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(os.path.join(DATATYPES[type], \"images\", f\"{name}.png\"))\n",
    "    ground_truth = np.load(os.path.join(DATATYPES[type], \"points\", f\"{name}.npy\"))\n",
    "    ground_truth = [cv2.KeyPoint(y, x, 1, 0, 0, 0) for x, y in ground_truth]\n",
    "    return (image, ground_truth)\n",
    "\n",
    "\n",
    "def rotate_image(image: cv2.typing.MatLike, angle: int) -> cv2.typing.MatLike:\n",
    "    \"\"\"\n",
    "    Rotates an image by a given angle.\n",
    "\n",
    "    :param image: The image to rotate.\n",
    "    :param angle: The angle to rotate the image by.\n",
    "\n",
    "    :return rotated_image: The rotated image.\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    matrix_rotation = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1)\n",
    "    rotated_image = cv2.warpAffine(image, matrix_rotation, (w, h))\n",
    "    return rotated_image\n",
    "\n",
    "\n",
    "def rotate_keypoints(\n",
    "    size: Tuple[int, int], keypoints: List[cv2.KeyPoint], angle: int\n",
    ") -> List[cv2.KeyPoint]:\n",
    "    \"\"\"\n",
    "    Rotates the keypoints of an image.\n",
    "\n",
    "    :param size: The size of the image (witdh, height).\n",
    "    :param keypoints: The keypoints to rotate.\n",
    "    :param angle: The angle to rotate the keypoints by.\n",
    "\n",
    "    :return (result, idx): A tuple containing the rotated keypoints and their indices in the original list.\n",
    "    \"\"\"\n",
    "    matrix_rotation = cv2.getRotationMatrix2D((size[0] / 2, size[1] / 2), angle, 1)\n",
    "    kps = np.array([[kp.pt[0], kp.pt[1]] for kp in keypoints])\n",
    "    kps = np.concatenate([kps, np.ones((len(kps), 1))], axis=1)\n",
    "    rotated_kps = np.array(np.dot(matrix_rotation, kps.T)).T\n",
    "\n",
    "    result, idx = [], []\n",
    "    for i in range(len(rotated_kps)):\n",
    "        kp = rotated_kps[i]\n",
    "        if 0 <= kp[0] < size[0] and 0 <= kp[1] < size[1]:\n",
    "            result.append(cv2.KeyPoint(kp[0], kp[1], 1, 0, 0, 0))\n",
    "            idx.append(i)\n",
    "\n",
    "    return (result, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "bf_sift = cv2.BFMatcher()\n",
    "\n",
    "for type in [0, 1, 3, 4, 5, 6, 7]:\n",
    "    image, ground_truth = read_image(type, \"1\")\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    for angle in [10]:\n",
    "        rotated_image = rotate_image(image, angle)\n",
    "        rotated_gray_scale = cv2.cvtColor(rotated_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        h, w = rotated_gray_scale.shape\n",
    "        rotated_keypoints, idx = rotate_keypoints((w, h), ground_truth, angle)\n",
    "\n",
    "        original_keypoints = [ground_truth[i] for i in idx]\n",
    "        original_descriptors = sift.compute(gray_scale, original_keypoints)[1]\n",
    "        rotated_descriptors = sift.compute(rotated_gray_scale, rotated_keypoints)[1]\n",
    "\n",
    "        matches = bf_sift.knnMatch(original_descriptors, rotated_descriptors, k=2)\n",
    "        if len(matches) == 0 or len(matches[0]) < 2:\n",
    "            continue\n",
    "\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.75 * n.distance and m.trainIdx == m.queryIdx:\n",
    "                good_matches.append(m)\n",
    "\n",
    "        image_matches = cv2.drawMatches(\n",
    "            image, ground_truth, rotated_image, rotated_keypoints, good_matches, None\n",
    "        )\n",
    "        cv2.imwrite(\n",
    "            f\"./services/keypoint_matching/results/sift_{type}_{angle}.png\",\n",
    "            image_matches,\n",
    "        )\n",
    "        np.save(\n",
    "            f\"./services/keypoint_matching/results/sift_{type}_{angle}.npy\",\n",
    "            np.array([len(good_matches), len(original_keypoints)]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = cv2.ORB_create()\n",
    "bf_orb = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "for type in [0, 1, 3, 4, 5, 6, 7]:\n",
    "    image, ground_truth = read_image(type, \"1\")\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    for angle in [10]:\n",
    "        rotated_image = rotate_image(image, angle)\n",
    "        rotated_gray_scale = cv2.cvtColor(rotated_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        h, w = rotated_gray_scale.shape\n",
    "        rotated_keypoints, idx = rotate_keypoints((w, h), ground_truth, angle)\n",
    "\n",
    "        original_keypoints = [ground_truth[i] for i in idx]\n",
    "        original_descriptors = orb.compute(gray_scale, original_keypoints)[1]\n",
    "        rotated_descriptors = orb.compute(rotated_gray_scale, rotated_keypoints)[1]\n",
    "\n",
    "        matches = bf_orb.match(original_descriptors, rotated_descriptors)\n",
    "        matches = [m for m in matches if m.queryIdx == m.trainIdx]\n",
    "        image_matches = cv2.drawMatches(\n",
    "            image, ground_truth, rotated_image, rotated_keypoints, matches, None\n",
    "        )\n",
    "\n",
    "        cv2.imwrite(\n",
    "            f\"./services/keypoint_matching/results/orb_{type}_{angle}.png\",\n",
    "            image_matches,\n",
    "        )\n",
    "        np.save(\n",
    "            f\"./services/keypoint_matching/results/orb_{type}_{angle}.npy\",\n",
    "            np.array([len(matches), len(original_keypoints)]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hazzu/Code/opencv_streamlit/services/keypoint_matching/superpoint.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hazzu/anaconda3/envs/opencv_streamlit/lib/python3.12/site-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from services.keypoint_matching.superpoint import SuperPointFrontend\n",
    "\n",
    "\n",
    "fe = SuperPointFrontend(\n",
    "    weights_path=\"services/keypoint_matching/superpoint_v1.pth\",\n",
    "    nms_dist=4,\n",
    "    conf_thresh=0.015,\n",
    "    nn_thresh=0.7,\n",
    "    cuda=True,\n",
    ")\n",
    "bf_fe = cv2.BFMatcher()\n",
    "\n",
    "for type in [0, 1, 3, 4, 5, 6, 7]:\n",
    "    image, ground_truth = read_image(type, \"1\")\n",
    "\n",
    "    for angle in [10]:\n",
    "        rotated_image = rotate_image(image, angle)\n",
    "        rotated_gray_scale = cv2.cvtColor(rotated_image, cv2.COLOR_BGR2GRAY)\n",
    "        rotated_gray_scale = rotated_gray_scale.astype(np.float32) / 255.0\n",
    "\n",
    "        h, w = rotated_gray_scale.shape\n",
    "        rotated_keypoints, idx = rotate_keypoints((w, h), ground_truth, angle)\n",
    "\n",
    "        original_keypoints = [ground_truth[i] for i in idx]\n",
    "        original_descriptors = fe.compute(rotated_gray_scale, original_keypoints)[1]\n",
    "        rotated_descriptors = fe.compute(rotated_gray_scale, rotated_keypoints)[1]\n",
    "\n",
    "        matches = bf_fe.knnMatch(original_descriptors, rotated_descriptors, k=2)\n",
    "        if len(matches) == 0 or len(matches[0]) < 2:\n",
    "            continue\n",
    "\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.75 * n.distance and m.trainIdx == m.queryIdx:\n",
    "                good_matches.append(m)\n",
    "\n",
    "        image_matches = cv2.drawMatches(\n",
    "            image, ground_truth, rotated_image, rotated_keypoints, good_matches, None\n",
    "        )\n",
    "        cv2.imwrite(\n",
    "            f\"./services/keypoint_matching/results/superpoint_{type}_{angle}.png\",\n",
    "            image_matches,\n",
    "        )\n",
    "        np.save(\n",
    "            f\"./services/keypoint_matching/results/superpoint_{type}_{angle}.npy\",\n",
    "            np.array([len(good_matches), len(original_keypoints)]),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv_streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
